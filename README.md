# Abstractive-summarization
This repository implements an abstractive text summarization model using the T5 transformer architecture. The model is trained on a dataset of news articles and their corresponding summaries.

Key Features:

Pre-trained T5 Model: Leverages the power of a pre-trained T5 model for strong performance.
Fine-Tuning: Fine-tunes the model on a specific summarization dataset to improve accuracy.
Data Preprocessing: Includes data cleaning, tokenization, and batching for efficient training.
Model Training: Implements a robust training pipeline using Hugging Face's Transformers library.
Model Evaluation: Evaluates the model's performance using metrics like ROUGE.
